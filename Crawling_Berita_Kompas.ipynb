{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ40HnbNgXXc"
      },
      "outputs": [],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Fungsi untuk mengambil link berita dari halaman Kompas Regional\n",
        "def get_news_links(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    if response.status_code == 200:\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "        # Mencari semua link berita di halaman\n",
        "        news_links = soup.find_all(\"a\", href=True)\n",
        "        # Filter hanya link yang mengandung 'regional.kompas.com/read/'\n",
        "        regional_news_links = [link['href'] for link in news_links if 'nasional.kompas.com/read/' in link['href']]\n",
        "        return regional_news_links\n",
        "    else:\n",
        "        print(f\"Gagal mengakses halaman. Status code: {response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# URL halaman utama Kompas Regional\n",
        "base_url = \"https://nasional.kompas.com/\"\n",
        "news_links = []\n",
        "page_num = 1\n",
        "max_pages = 10  # Maksimal halaman yang akan dibuka\n",
        "\n",
        "# Loop untuk mengambil link berita sampai mendapatkan 200 link atau maksimal halaman tercapai\n",
        "while len(news_links) < 1000 and page_num <= max_pages:\n",
        "    # URL untuk halaman dengan pagination (asumsi URLnya berformat seperti ini)\n",
        "    page_url = f\"{base_url}?page={page_num}\"\n",
        "    print(f\"Mengambil link dari halaman: {page_url}\")\n",
        "\n",
        "    # Mengambil link dari halaman saat ini\n",
        "    new_links = get_news_links(page_url)\n",
        "\n",
        "    if not new_links:\n",
        "        # Jika tidak ada link baru yang ditemukan, hentikan pencarian\n",
        "        break\n",
        "\n",
        "    news_links.extend(new_links)\n",
        "\n",
        "    # Menghilangkan duplikasi link\n",
        "    news_links = list(set(news_links))\n",
        "\n",
        "    # Meningkatkan nomor halaman untuk pagination\n",
        "    page_num += 1\n",
        "\n",
        "# Menyimpan link ke file CSV menggunakan pandas\n",
        "df = pd.DataFrame(news_links, columns=[\"link\"])\n",
        "df.to_csv('kompas_nasional.csv', index=False)\n",
        "\n",
        "print(f\"Total {len(news_links)} link telah disimpan ke 'news_links.csv'\")\n"
      ],
      "metadata": {
        "id": "nvGuA8AvghMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Fungsi untuk mengambil konten dari sebuah artikel\n",
        "def crawl_article(url):\n",
        "    url = url + '?page=all'  # menambahkan '?page=all' untuk mendapatkan seluruh konten tanpa paging\n",
        "    req_berita = requests.get(url)\n",
        "    soup_berita = BeautifulSoup(req_berita.text, 'lxml')\n",
        "\n",
        "    # Mengambil judul\n",
        "    try:\n",
        "        title = soup_berita.find_all('title')[0].text\n",
        "    except:\n",
        "        title = 'Tidak ada judul'\n",
        "\n",
        "    # Mengambil tanggal\n",
        "    try:\n",
        "        tgl = soup_berita.find_all('div', {'class': 'read__time'})[0].text\n",
        "        date = tgl.split(',')[0].split('-')[1].replace(' ', \"\")\n",
        "    except:\n",
        "        date = 'Tidak ada tanggal'\n",
        "\n",
        "    # Mengambil isi artikel\n",
        "    try:\n",
        "        berita = soup_berita.find_all('div', {'class': 'read__content'})[0].text.strip().replace('\\n', ' ')\n",
        "    except:\n",
        "        berita = 'Tidak ada konten'\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'date': date,\n",
        "        'content': berita\n",
        "    }\n",
        "\n",
        "# Membaca file CSV yang berisi daftar link\n",
        "#file_path = '/content/news_links_Makassar.csv'\n",
        "#file_path = '/content/news_links_Medan.csv'\n",
        "#file_path = '/content/news_links_Surabaya.csv'\n",
        "#file_path = '/content/news_links_Yogyakarta.csv'\n",
        "#file_path = '/content/news_links_bandung.csv'\n",
        "#file_path = '/content/news_links_denpasar.csv'\n",
        "file_path = '/content/kompas_nasional.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "article_links = df['link'].tolist()\n",
        "\n",
        "all_articles = []\n",
        "for link in article_links:\n",
        "    print(f\"Memproses: {link}\")\n",
        "    article_content = crawl_article(link)\n",
        "    all_articles.append(article_content)\n",
        "\n",
        "    print(f\"Title: {article_content['title']}\")\n",
        "    print(f\"Date: {article_content['date']}\")\n",
        "    print(f\"Content: {article_content['content'][:200]}...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "df_articles = pd.DataFrame(all_articles)\n",
        "\n",
        "output_file = '/content/kompas_nasional.xlsx'\n",
        "df_articles.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Total {len(all_articles)} artikel telah disimpan ke '{output_file}'\")"
      ],
      "metadata": {
        "id": "d6ZyKxLCg9ZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pj41MAl6vz7I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}